{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n9sq9k0q47An",
        "OnuTCQhQ5C26",
        "K5YnMjXs5HD4",
        "cFfUWCsx5LRI",
        "s5pBZk1K5dsD",
        "MmQsG-WvDlc8",
        "rqdR9T5MG7N5",
        "0iJpN2DeKN6J"
      ],
      "authorship_tag": "ABX9TyP+v+/89PTDwPXM5qpwtHdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xketh/Fatec_PLN/blob/main/Aula_03_Processamento_de_Texto_e_Pr%C3%A9_processamento_de_Dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Objetivo**\n",
        "___\n",
        "\n",
        "Explorar técnicas essenciais de pré-processamento de texto, como normalização, tokenização, remoção de ruído e stopwords, além de stemming e lematização, para otimizar a análise de dados textuais.\n"
      ],
      "metadata": {
        "id": "E-RgzTG03iBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-processamento de texto**\n",
        "---\n",
        "\n",
        "É uma etapa crucial no processamento de linguagem natural (PLN).\n",
        "\n",
        "Ele limpa e organiza o texto, deixando apenas informações relevantes para facilitar o trabalho dos algoritmos.\n",
        "\n",
        "Aqui estão as principais técnicas e etapas que tornam o texto mais limpo e estruturado, facilitando análises precisas!"
      ],
      "metadata": {
        "id": "-AvsxG29DF8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Normalização de Texto**\n",
        "---\n",
        "\n",
        "Uniformiza o texto para evitar variações desnecessárias.  \n",
        "- **Exemplos:**  \n",
        "  - Converte para minúsculas: \"Carro\" → \"carro\".  \n",
        "  - Remove acentos: \"ação\" → \"acao\".  \n",
        "  - Expande contrações: \"não é\" → \"nao e\".  \n"
      ],
      "metadata": {
        "id": "n9sq9k0q47An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    # 1. Converter para minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 2. Remover acentos\n",
        "    texto = ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', texto)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "    # 3. Expandir contrações simples (exemplo básico)\n",
        "    contrações = {\n",
        "        \"não é\": \"nao e\",\n",
        "        \"não está\": \"nao esta\",\n",
        "        \"não\": \"nao\",\n",
        "        \"é\": \"e\"\n",
        "    }\n",
        "    for contração, expansão in contrações.items():\n",
        "        texto = texto.replace(contração, expansão)\n",
        "\n",
        "    return texto\n",
        "\n",
        "# Exemplo de uso:\n",
        "texto_original = \"Não é fácil entender a ação rápida do carro.\"\n",
        "texto_normalizado = normalizar_texto(texto_original)\n",
        "\n",
        "print(\"Original:\", texto_original)\n",
        "print(\"Normalizado:\", texto_normalizado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgY_8ANg6KGz",
        "outputId": "b66bd992-7fa7-4903-88f6-b9ce2533fa54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Não é fácil entender a ação rápida do carro.\n",
            "Normalizado: nao e facil entender a acao rapida do carro.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Remoção de Ruído**  \n",
        "---\n",
        "Elimina elementos que não agregam valor, como:  \n",
        "- **Exemplos:**  \n",
        "  - Pontuação: \"Olá, tudo bem?\" → \"Olá tudo bem\".  \n",
        "  - Links: \"Visite https://site.com\" → \"Visite\".  "
      ],
      "metadata": {
        "id": "OnuTCQhQ5C26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "texto = \"Olá, tudo bem? Visite https://site.com para mais informações!\"\n",
        "\n",
        "# Remove URLs\n",
        "texto_sem_links = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto)\n",
        "\n",
        "# Remove pontuação\n",
        "texto_limpo = re.sub(r'[^\\w\\s]', '', texto_sem_links)\n",
        "\n",
        "print(texto_limpo.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epEzW4JC6TXp",
        "outputId": "e5c234b2-3f35-49a1-f218-f5d8dc088e01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá tudo bem Visite  para mais informações\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3. Tokenização**  \n",
        "---\n",
        "Divide o texto em unidades menores (*tokens*), como palavras.  \n",
        "- **Exemplo:**  \n",
        "  - Texto: \"Eu gosto de aprender.\"  \n",
        "  - Tokens: [\"Eu\", \"gosto\", \"de\", \"aprender\", \".\"]  \n",
        "\n"
      ],
      "metadata": {
        "id": "K5YnMjXs5HD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Baixa os dados necessários para tokenização\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texto = \"Eu gosto de aprender.\"\n",
        "tokens = word_tokenize(texto)\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjDxanXa7qmi",
        "outputId": "0b5b4c62-686b-4fb1-afd0-26403fd64226"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Eu', 'gosto', 'de', 'aprender', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **4. Remoção de Stopwords**\n",
        "---\n",
        "\n",
        "Remove palavras comuns que pouco contribuem para a análise.  \n",
        "- **Exemplo:**  \n",
        "  - Antes: \"Eu gosto de aprender coisas novas.\"  \n",
        "  - Depois: [\"gosto\", \"aprender\", \"coisas\", \"novas\"].  "
      ],
      "metadata": {
        "id": "cFfUWCsx5LRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Baixar recursos necessários\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Frase original\n",
        "frase = \"Eu gosto de aprender coisas novas.\"\n",
        "\n",
        "# Tokenizar a frase\n",
        "tokens = word_tokenize(frase, language='portuguese')\n",
        "\n",
        "# Definir stopwords em português\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Remover stopwords\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt and palavra.isalpha()]\n",
        "\n",
        "print(\"Antes:\", tokens)\n",
        "print(\"Depois:\", tokens_sem_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DVYZ_ma8ZU-",
        "outputId": "0636c90a-bdac-404e-da23-cdb0ef583206"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antes: ['Eu', 'gosto', 'de', 'aprender', 'coisas', 'novas', '.']\n",
            "Depois: ['gosto', 'aprender', 'coisas', 'novas']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **5. Stemming**  \n",
        "---\n",
        "\n",
        "Reduz palavras à sua raiz, sem considerar o contexto.  \n",
        "- **Exemplo:**  \n",
        "  - \"Correndo\", \"Correr\", \"Correu\" → \"Corr\".  \n",
        "  - \"Amando\", \"Amar\", \"Amei\" → \"Am\".  \n",
        "\n"
      ],
      "metadata": {
        "id": "s5pBZk1K5dsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "# Baixar recursos necessários\n",
        "nltk.download('rslp')\n",
        "\n",
        "# Inicializa o stemmer para português\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "# Palavras de exemplo\n",
        "palavras = ['correndo', 'correr', 'correu', 'amando', 'amar', 'amei']\n",
        "\n",
        "# Aplica o stemming\n",
        "raízes = [stemmer.stem(palavra) for palavra in palavras]\n",
        "\n",
        "# Resultado\n",
        "for palavra, raiz in zip(palavras, raízes):\n",
        "    print(f'{palavra}  ->  {raiz}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeY3uGR28oCZ",
        "outputId": "44d1e548-7b41-495a-90a0-9a041d905492"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correndo  ->  corr\n",
            "correr  ->  corr\n",
            "correu  ->  corr\n",
            "amando  ->  am\n",
            "amar  ->  am\n",
            "amei  ->  ame\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **6. Lematização**  \n",
        "---\n",
        "\n",
        "Reduz palavras à forma base, considerando o contexto.  \n",
        "- **Exemplo:**  \n",
        "  - \"Correndo\", \"Correr\", \"Correu\" → \"Correr\".  \n",
        "  - \"Melhores\" → \"Melhor\".  \n",
        "\n",
        "\n",
        "#### **Por que a diferença entre inglês e português na lematização?**\n",
        "\n",
        "Cada língua tem sua própria gramática, estrutura e variações de palavras. Por isso, os programas que fazem lematização precisam “entender” essas regras específicas para funcionar bem.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wGXC72P15mks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6.1 Lematização em Inglês**\n",
        "\n",
        "Este código é um exemplo de lematização usando o **WordNetLemmatizer** da biblioteca **nltk**, que funciona bem porque o **inglês** é mais simples estruturalmente.\n",
        "\n",
        "**Resumindo o código**\n",
        "\n",
        "* **A Palavra 1**, 'running' (correndo) é reduzida para a forma base 'run' (correr), já que foi especificado que é um verbo (pos='v').\n",
        "\n",
        "* **A Palavra 2**, 'better' (melhor) é reduzida para 'good' (bom), já que foi especificado que é um adjetivo (pos='a')."
      ],
      "metadata": {
        "id": "MmQsG-WvDlc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lematizando palavras simples\n",
        "print(lemmatizer.lemmatize('running', pos='v'))  # Forma base: 'run'\n",
        "print(lemmatizer.lemmatize('better', pos='a'))   # Forma base: 'good'\n"
      ],
      "metadata": {
        "id": "ZbL2G3Y6GiaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6.2 Lematização em Português**\n",
        "\n",
        "Este código é um exemplo de lematização usando o **SpaCy** uma biblioteca mais moderna, rápida e eficiente, com modelos treinados para várias línguas, inclusive o português. Ela entende melhor a gramática complexa de idiomas como o português, por isso é indicada para trabalhos mais avançados ou em línguas diferentes do inglês.\n",
        "\n",
        "* **Antes de carregar o modelo, instale:**\n",
        "\n",
        " !python -m spacy download pt_core_news_sm\n",
        "\n",
        "\n",
        "**Depois execute**\n"
      ],
      "metadata": {
        "id": "rqdR9T5MG7N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UK03sRXIZYX",
        "outputId": "592e352b-2f0d-4925-c2a4-9f5462db10ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Baixe o modelo para português uma vez: !python -m spacy download pt_core_news_sm\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Lematizando palavras simples\n",
        "texto = \"correndo melhor\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text} -> Lema: {token.lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjOduV0LIWf7",
        "outputId": "8183b026-038b-45a5-a767-b448bec82c1d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavra: correndo -> Lema: correr\n",
            "Palavra: melhor -> Lema: bom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos de linguagem**\n",
        "---\n",
        "Imagine um programa que \"aprendeu\" a entender e trabalhar com textos.\n",
        "\n",
        "Esses modelos funcionam como \"cérebros digitais\" treinados para realizar tarefas específicas de linguagem, como identificar palavras importantes, corrigir erros ou resumir textos.\n",
        "\n",
        "Eles são criados a partir de:\n",
        "\n",
        "* Conjuntos de dados estatísticos e regras.\n",
        "\n",
        "* Treinamento com milhões de textos.\n",
        "\n",
        "* Especialização para tarefas específicas, como análise de sentimentos ou tradução.\n",
        "\n",
        "* Aprendizado de máquina, que ensina o modelo a melhorar com base nos dados fornecidos.\n",
        "\n"
      ],
      "metadata": {
        "id": "kuowP9xP_Fj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exemplo (em português)**\n",
        "\n",
        "O código a seguir mostra como modelos de linguagem pré-treinados podem realizar tarefas úteis automaticamente, como:\n",
        "\n",
        "* Identificar palavras e suas funções.\n",
        "\n",
        "* Reconhecer entidades e conceitos.\n",
        "\n",
        "* Dividir textos em partes mais simples para análise."
      ],
      "metadata": {
        "id": "0iJpN2DeKN6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carregar o modelo para português\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Texto simples sobre o tema pedido\n",
        "texto = \"Estou super atrasada para entregar as atividades dessa aula da FATEC. Preciso correr para terminar tudo a tempo.\"\n",
        "\n",
        "# Processar o texto com o modelo\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Mostrar palavras e suas funções (posição gramatical)\n",
        "print(\"Palavras e suas funções:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.pos_}\")\n",
        "\n",
        "# Mostrar as frases do texto\n",
        "print(\"\\nFrases:\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSa5ThkoK7Od",
        "outputId": "e3d97925-da8e-4105-e743-6ca27026f2be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras e suas funções:\n",
            "Estou -> AUX\n",
            "super -> ADJ\n",
            "atrasada -> VERB\n",
            "para -> SCONJ\n",
            "entregar -> VERB\n",
            "as -> DET\n",
            "atividades -> NOUN\n",
            "dessa -> ADP\n",
            "aula -> NOUN\n",
            "da -> ADP\n",
            "FATEC -> PROPN\n",
            ". -> PUNCT\n",
            "Preciso -> VERB\n",
            "correr -> VERB\n",
            "para -> SCONJ\n",
            "terminar -> VERB\n",
            "tudo -> PRON\n",
            "a -> ADP\n",
            "tempo -> NOUN\n",
            ". -> PUNCT\n",
            "\n",
            "Frases:\n",
            "Estou super atrasada para entregar as atividades dessa aula da FATEC.\n",
            "Preciso correr para terminar tudo a tempo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Como são treinados?**\n",
        "---\n",
        "* Alimentam o modelo com grandes volumes de textos (livros, sites, artigos).\n",
        "\n",
        "* Ele aprende padrões do idioma, como conjugações e uso de palavras.\n",
        "\n",
        "* Reconhece estruturas gramaticais, como onde usar verbos ou substantivos.\n",
        "\n",
        "* Entende relações entre palavras, como \"correr\" e \"correndo\" serem da mesma ação.\n",
        "\n",
        "* Depois, ele é testado e ajustado até funcionar bem."
      ],
      "metadata": {
        "id": "Vnr0_PoNADUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tipos de modelos por tamanho**\n",
        "---\n",
        "Os modelos têm diferentes tamanhos, dependendo da rapidez e precisão necessárias:\n",
        "\n",
        "* **Pequeno (sm):** É rápido, usa pouca memória, mas menos preciso. Bom para testes rápidos.\n",
        "\n",
        "* **Médio (md):** Equilibra velocidade e precisão. Indicado para tarefas do dia a dia.\n",
        "\n",
        "* **Grande (lg):** É o mais preciso, mas também o mais lento e usa mais memória. Ideal para análises mais complexas."
      ],
      "metadata": {
        "id": "T1uHgi8QARW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos pré-treinados disponíveis no spaCy**\n",
        "---\n",
        "**Português:** nlp_pt = spacy.load('pt_core_news_sm')\n",
        "\n",
        "**Inglês:** nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "**Espanhol:** nlp_es = spacy.load('es_core_news_sm')\n",
        "\n",
        "**Francês:** nlp_fr = spacy.load('fr_core_news_sm')\n",
        "\n",
        "**Alemão:** nlp_de = spacy.load('de_core_news_sm')\n",
        "\n",
        "Esses modelos já vêm \"treinados\" para entender cada idioma, facilitando tarefas como análise de texto, lematização ou extração de palavras-chave."
      ],
      "metadata": {
        "id": "lXUl1sNPA7a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exemplo em Inglês (usando en_core_web_sm)**"
      ],
      "metadata": {
        "id": "BEGmpjqqLxqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carrega o modelo pré-treinado para inglês\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "texto_en = \"I am late to submit the assignments for class at FATEC. I need to hurry to finish everything on time.\"\n",
        "\n",
        "doc_en = nlp_en(texto_en)\n",
        "\n",
        "print(\"English - Words and their parts of speech:\")\n",
        "for token in doc_en:\n",
        "    print(f\"{token.text} -> {token.pos_}\")\n",
        "\n",
        "print(\"\\nEnglish - Sentences:\")\n",
        "for sent in doc_en.sents:\n",
        "    print(sent.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POogo7mcL_fr",
        "outputId": "2d8e4c2d-f4ca-46f0-bdd4-f7fe0103e174"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English - Words and their parts of speech:\n",
            "I -> PRON\n",
            "am -> AUX\n",
            "late -> ADJ\n",
            "to -> PART\n",
            "submit -> VERB\n",
            "the -> DET\n",
            "assignments -> NOUN\n",
            "for -> ADP\n",
            "class -> NOUN\n",
            "at -> ADP\n",
            "FATEC -> PROPN\n",
            ". -> PUNCT\n",
            "I -> PRON\n",
            "need -> VERB\n",
            "to -> PART\n",
            "hurry -> VERB\n",
            "to -> PART\n",
            "finish -> VERB\n",
            "everything -> PRON\n",
            "on -> ADP\n",
            "time -> NOUN\n",
            ". -> PUNCT\n",
            "\n",
            "English - Sentences:\n",
            "I am late to submit the assignments for class at FATEC.\n",
            "I need to hurry to finish everything on time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exemplo em Espanhol**"
      ],
      "metadata": {
        "id": "wrKDSwtKMYz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar a biblioteca e baixar o modelo do idioma\n",
        "\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HVefg2GMsqU",
        "outputId": "9fb368f2-54c8-45aa-8d40-03516b539601"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carrega o modelo para espanhol\n",
        "nlp_es = spacy.load('es_core_news_sm')\n",
        "\n",
        "texto_es = \"Estoy atrasado para entregar las tareas de la clase en la universidad. Necesito apurarme para terminar todo a tiempo.\"\n",
        "\n",
        "doc_es = nlp_es(texto_es)\n",
        "\n",
        "print(\"Palabras y sus funciones gramaticales (Español):\")\n",
        "for token in doc_es:\n",
        "    print(f\"{token.text} -> {token.pos_}\")\n",
        "\n",
        "print(\"\\nOraciones (Español):\")\n",
        "for sent in doc_es.sents:\n",
        "    print(sent.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTt-Ak1EMhAf",
        "outputId": "cf373f49-9724-4e19-da89-0df737772803"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras y sus funciones gramaticales (Español):\n",
            "Estoy -> AUX\n",
            "atrasado -> ADJ\n",
            "para -> ADP\n",
            "entregar -> VERB\n",
            "las -> DET\n",
            "tareas -> NOUN\n",
            "de -> ADP\n",
            "la -> DET\n",
            "clase -> NOUN\n",
            "en -> ADP\n",
            "la -> DET\n",
            "universidad -> NOUN\n",
            ". -> PUNCT\n",
            "Necesito -> VERB\n",
            "apurarme -> VERB\n",
            "para -> ADP\n",
            "terminar -> VERB\n",
            "todo -> PRON\n",
            "a -> ADP\n",
            "tiempo -> NOUN\n",
            ". -> PUNCT\n",
            "\n",
            "Oraciones (Español):\n",
            "Estoy atrasado para entregar las tareas de la clase en la universidad.\n",
            "Necesito apurarme para terminar todo a tiempo.\n"
          ]
        }
      ]
    }
  ]
}